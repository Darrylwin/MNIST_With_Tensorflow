"""
DÃ‰MO MNIST - Version SimplifiÃ©e pour PrÃ©sentation
Groupe 11 - Big Data - Deep Learning
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import os

# DÃ©sactiver les warnings TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def afficher_titre(texte):
    """Affiche un titre formatÃ©"""
    print("\n" + "="*70)
    print(f"  {texte}")
    print("="*70)

def afficher_section(texte):
    """Affiche une section"""
    print(f"\nğŸ”¹ {texte}")

# ============================================================================
# DÃ‰BUT DE LA DÃ‰MO
# ============================================================================

afficher_titre("DÃ‰MO MNIST - RECONNAISSANCE DE CHIFFRES MANUSCRITS")
print("\nğŸ‘¥ Groupe 11 : DEGBEY, DOSSOU, DOUFFAN, LOGOSSOU")
print("ğŸ“š Sujet : Deep Learning et RÃ©seaux Neuronaux")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour commencer la dÃ©mo...")

# ============================================================================
# Ã‰TAPE 1 : CHARGEMENT DES DONNÃ‰ES
# ============================================================================

afficher_section("Ã‰TAPE 1 : Chargement du dataset MNIST")
print("   Le dataset MNIST contient 70,000 images de chiffres manuscrits (0-9)")

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(f"\n   âœ… DonnÃ©es chargÃ©es !")
print(f"   ğŸ“Š {x_train.shape[0]:,} images pour l'entraÃ®nement")
print(f"   ğŸ“Š {x_test.shape[0]:,} images pour le test")
print(f"   ğŸ“ Taille de chaque image : {x_train.shape[1]}x{x_train.shape[2]} pixels")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour voir quelques exemples d'images...")

# Afficher quelques exemples
plt.figure(figsize=(12, 4))
plt.suptitle("Exemples d'images du dataset MNIST", fontsize=14, fontweight='bold')
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(f"Chiffre : {y_train[i]}", fontsize=10)
    plt.axis('off')
plt.tight_layout()
plt.savefig('1_exemples_mnist.png', dpi=150)
print("\n   ğŸ’¾ Image sauvegardÃ©e : 1_exemples_mnist.png")
plt.show(block=False)
plt.pause(2)

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour passer au prÃ©traitement...")

# ============================================================================
# Ã‰TAPE 2 : PRÃ‰TRAITEMENT
# ============================================================================

afficher_section("Ã‰TAPE 2 : PrÃ©traitement des donnÃ©es")
print("   Transformation nÃ©cessaire avant l'entraÃ®nement :")
print("   1ï¸âƒ£  Normalisation : pixels 0-255 â†’ 0-1")
print("   2ï¸âƒ£  Aplatissement : images 28x28 â†’ vecteurs de 784")

# Normalisation
x_train = x_train / 255.0
x_test = x_test / 255.0

# Aplatissement
x_train_flat = x_train.reshape(-1, 784)
x_test_flat = x_test.reshape(-1, 784)

print(f"\n   âœ… PrÃ©traitement terminÃ© !")
print(f"   ğŸ“ Nouvelle forme des donnÃ©es : {x_train_flat.shape}")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour construire le rÃ©seau de neurones...")

# ============================================================================
# Ã‰TAPE 3 : CONSTRUCTION DU MODÃˆLE
# ============================================================================

afficher_section("Ã‰TAPE 3 : Construction du rÃ©seau de neurones")
print("   Architecture du modÃ¨le :")
print("   ğŸ”´ Couche d'entrÃ©e : 784 neurones (28x28 pixels)")
print("   ğŸŸ  Couche cachÃ©e 1 : 128 neurones + ReLU")
print("   ğŸŸ¡ Dropout : 20% (Ã©vite le surapprentissage)")
print("   ğŸŸ¢ Couche cachÃ©e 2 : 64 neurones + ReLU")
print("   ğŸ”µ Couche de sortie : 10 neurones + Softmax (chiffres 0-9)")

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

# Compilation
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("\n   âœ… ModÃ¨le crÃ©Ã© et compilÃ© !")
print(f"   ğŸ§® ParamÃ¨tres Ã  entraÃ®ner : {model.count_params():,}")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour lancer l'entraÃ®nement...")

# ============================================================================
# Ã‰TAPE 4 : ENTRAÃNEMENT
# ============================================================================

afficher_section("Ã‰TAPE 4 : EntraÃ®nement du modÃ¨le")
print("   â±ï¸  Cela prendra environ 1-2 minutes...")
print("   ğŸ“ˆ Suivez l'Ã©volution de la prÃ©cision (accuracy)\n")

history = model.fit(
    x_train_flat, 
    y_train,
    epochs=5,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

print("\n   âœ… EntraÃ®nement terminÃ© !")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour Ã©valuer les performances...")

# ============================================================================
# Ã‰TAPE 5 : Ã‰VALUATION
# ============================================================================

afficher_section("Ã‰TAPE 5 : Ã‰valuation sur les donnÃ©es de test")

test_loss, test_accuracy = model.evaluate(x_test_flat, y_test, verbose=0)

print(f"\n   ğŸ¯ RÃ‰SULTATS FINAUX :")
print(f"   {'â”€'*50}")
print(f"   PrÃ©cision (Accuracy) : {test_accuracy*100:.2f}%")
print(f"   Perte (Loss)         : {test_loss:.4f}")
print(f"   {'â”€'*50}")

if test_accuracy > 0.97:
    print("   ğŸ† Excellent rÃ©sultat ! Le modÃ¨le est trÃ¨s performant.")
elif test_accuracy > 0.95:
    print("   âœ¨ Bon rÃ©sultat ! Le modÃ¨le fonctionne bien.")
else:
    print("   âš ï¸  Le modÃ¨le pourrait Ãªtre amÃ©liorÃ©.")

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour voir les graphiques...")

# CrÃ©er les graphiques
plt.figure(figsize=(14, 5))

# Graphique 1 : PrÃ©cision
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], 'b-', label='EntraÃ®nement', linewidth=2)
plt.plot(history.history['val_accuracy'], 'r-', label='Validation', linewidth=2)
plt.title('Ã‰volution de la PrÃ©cision', fontsize=14, fontweight='bold')
plt.xlabel('Ã‰poque', fontsize=12)
plt.ylabel('PrÃ©cision (%)', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# Graphique 2 : Perte
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], 'b-', label='EntraÃ®nement', linewidth=2)
plt.plot(history.history['val_loss'], 'r-', label='Validation', linewidth=2)
plt.title('Ã‰volution de la Perte', fontsize=14, fontweight='bold')
plt.xlabel('Ã‰poque', fontsize=12)
plt.ylabel('Perte', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('2_courbes_apprentissage.png', dpi=150)
print("\n   ğŸ’¾ Graphiques sauvegardÃ©s : 2_courbes_apprentissage.png")
plt.show(block=False)
plt.pause(2)

input("\nâ–¶ï¸  Appuyez sur ENTRÃ‰E pour tester le modÃ¨le sur de nouvelles images...")

# ============================================================================
# Ã‰TAPE 6 : PRÃ‰DICTIONS
# ============================================================================

afficher_section("Ã‰TAPE 6 : Test de prÃ©diction")
print("   Le modÃ¨le va maintenant prÃ©dire des chiffres qu'il n'a jamais vus\n")

# Faire 10 prÃ©dictions alÃ©atoires
plt.figure(figsize=(15, 6))
plt.suptitle("PrÃ©dictions du ModÃ¨le sur de Nouvelles Images", fontsize=14, fontweight='bold')

correct = 0
for i in range(10):
    idx = np.random.randint(0, len(x_test))
    image = x_test[idx]
    prediction = model.predict(x_test_flat[idx:idx+1], verbose=0)
    predicted_digit = np.argmax(prediction)
    true_digit = y_test[idx]
    confidence = prediction[0][predicted_digit] * 100
    
    if predicted_digit == true_digit:
        correct += 1
        color = 'green'
        status = "âœ“"
    else:
        color = 'red'
        status = "âœ—"
    
    print(f"   {status} Test {i+1:2d} : PrÃ©dit = {predicted_digit}, RÃ©el = {true_digit}, Confiance = {confidence:.1f}%")
    
    plt.subplot(2, 5, i+1)
    plt.imshow(image, cmap='gray')
    plt.title(f'P:{predicted_digit} | R:{true_digit}\n{confidence:.0f}%', 
              color=color, fontsize=10, fontweight='bold')
    plt.axis('off')

plt.tight_layout()
plt.savefig('3_predictions.png', dpi=150)
print(f"\n   ğŸ’¾ PrÃ©dictions sauvegardÃ©es : 3_predictions.png")
print(f"   ğŸ“Š RÃ©ussite : {correct}/10 prÃ©dictions correctes")
plt.show(block=False)
plt.pause(2)

# ============================================================================
# CONCLUSION
# ============================================================================

afficher_titre("CONCLUSION")
print("""
âœ… Ce que nous avons dÃ©montrÃ© :

1. ğŸ“¥ Chargement et exploration d'un dataset rÃ©el (MNIST)
2. ğŸ”§ PrÃ©traitement des donnÃ©es pour le deep learning
3. ğŸ§  Construction d'un rÃ©seau de neurones avec plusieurs couches
4. ğŸš€ EntraÃ®nement du modÃ¨le (apprentissage des patterns)
5. ğŸ“Š Ã‰valuation des performances (~98% de prÃ©cision)
6. ğŸ”® Utilisation du modÃ¨le pour faire des prÃ©dictions

ğŸ¯ POINTS CLÃ‰S :
   â€¢ Un rÃ©seau simple avec 3 couches suffit pour atteindre 98% de prÃ©cision
   â€¢ Le modÃ¨le apprend automatiquement Ã  reconnaÃ®tre les chiffres
   â€¢ L'entraÃ®nement prend quelques minutes sur un PC standard
   â€¢ Le deep learning est accessible et pratique !

ğŸ“š Ce modÃ¨le illustre les concepts fondamentaux du deep learning
   que nous avons prÃ©sentÃ©s dans notre exposÃ©.
""")

afficher_titre("FIN DE LA DÃ‰MO - MERCI !")
print("\nğŸ‘¥ Groupe 11 : DEGBEY, DOSSOU, DOUFFAN, LOGOSSOU")
print("ğŸ“§ Questions ? N'hÃ©sitez pas !\n")

input("â–¶ï¸  Appuyez sur ENTRÃ‰E pour fermer...")
plt.close('all')